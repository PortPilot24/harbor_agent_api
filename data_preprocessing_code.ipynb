{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4QBhstdxViA",
        "outputId": "c2da604b-a2a1-4f8a-abbe-0667bac08a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers>=4.32.0 in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m583.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.32.0) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.32.0) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.32.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.32.0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.32.0) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.32.0) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m835.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=cc11f6d538a84099738b7b49e4eaecd12e714d09d079806aced8e0429446d7d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, XlsxWriter, uvloop, python-dotenv, python-docx, pymupdf, pybase64, overrides, opentelemetry-proto, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, python-pptx, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, nvidia-cusolver-cu12, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed XlsxWriter-3.2.5 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pymupdf-1.26.3 pypika-0.48.9 python-docx-1.2.0 python-dotenv-1.1.1 python-pptx-1.0.2 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx chromadb sentence-transformers \"transformers>=4.32.0\" \"torch>=2.0.0\" \"accelerate\" pymupdf python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XusoWhm2xUhE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import docx\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import sys\n",
        "import hashlib\n",
        "import gc\n",
        "from typing import List, Dict, Any, Optional, Tuple, Union, Set\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from docx.opc.exceptions import PackageNotFoundError\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "import json\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import sqlite3\n",
        "from contextlib import contextmanager\n",
        "import uuid\n",
        "import functools\n",
        "from queue import Queue\n",
        "import multiprocessing as mp\n",
        "\n",
        "# DOCX\n",
        "from docx.document import Document\n",
        "from docx.table import Table as DocxTable\n",
        "from docx.text.paragraph import Paragraph\n",
        "from docx.oxml.text.paragraph import CT_P\n",
        "from docx.oxml.table import CT_Tbl\n",
        "\n",
        "# PDF\n",
        "import fitz\n",
        "\n",
        "# PPTX\n",
        "from pptx import Presentation\n",
        "from pptx.table import Table as PptxTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjkAWTCqy1v3"
      },
      "outputs": [],
      "source": [
        "# 로깅 설정\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('/content/legal_processing.log', encoding='utf-8')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHf3QuIEy4yx"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class LegalStructure:\n",
        "    \"\"\"법률 구조 정보를 담는 데이터 클래스\"\"\"\n",
        "    structure_type: str\n",
        "    number: str\n",
        "    title: str\n",
        "    level: int\n",
        "    parent_ref: Optional[str] = None\n",
        "    cross_refs: List[str] = field(default_factory=list)\n",
        "    original_chunk_id: Optional[str] = None  # 원본 청크 추적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPJljeUky6NE"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"청크 메타데이터 확장\"\"\"\n",
        "    source_file: str\n",
        "    file_hash: str\n",
        "    title: str\n",
        "    structure_info: Dict[str, Any]\n",
        "    hierarchy_path: List[str]\n",
        "    parent_structures: List[Dict[str, str]]\n",
        "    char_count: int\n",
        "    word_count: int\n",
        "    chunk_index: int\n",
        "    is_sub_chunk: bool = False\n",
        "    sub_index: int = 0\n",
        "    parent_chunk_id: Optional[str] = None  # 분할된 청크의 원본 추적\n",
        "    creation_time: datetime = field(default_factory=datetime.now)\n",
        "    last_updated: datetime = field(default_factory=datetime.now)\n",
        "    effective_date: Optional[str] = None\n",
        "    publication_date: Optional[str] = None\n",
        "    amendment_info: Optional[str] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nQZGjUjy-EO"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"문서 처리 파이프라인에 대한 구성 설정\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_data_directory() -> str:\n",
        "        \"\"\"환경에 따른 데이터 디렉토리 자동 감지\"\"\"\n",
        "        if 'COLAB_RELEASE_TAG' in os.environ:\n",
        "            colab_path = \"/content/drive/MyDrive/pj/test\"\n",
        "            # colab_path = \"/content/drive/MyDrive/pj/data\"\n",
        "            if os.path.exists(\"/content/drive\"):\n",
        "                return colab_path\n",
        "        return os.path.join(os.getcwd(), \"data\")\n",
        "\n",
        "    DOCX_DIRECTORY: str = get_data_directory()\n",
        "    DB_PATH: str = \"/content/chroma_db\"\n",
        "    COLLECTION_NAME: str = \"legal_manuals\"\n",
        "    # COLLECTION_NAME: str = \"legal_docs\"\n",
        "    METADATA_DB_PATH: str = \"/content/legal_metadata.db\"\n",
        "\n",
        "    # 법률 특화 임베딩 모델 설정\n",
        "    EMBEDDING_MODELS: List[str] = [\n",
        "        \"jhgan/ko-sroberta-multitask\",\n",
        "        # \"BAAI/bge-m3\",\n",
        "        # \"nlpai-lab/KURE\",\n",
        "        # \"intfloat/multilingual-e5-large-instruct\",\n",
        "    ]\n",
        "    PRIMARY_MODEL: str = \"jhgan/ko-sroberta-multitask\"\n",
        "    # LEGAL_SPECIALIZED_MODEL: str = \"bongsoo/kpf-bert-base\"  # 법률 특화 모델\n",
        "\n",
        "    METADATA: Dict[str, str] = {\"hnsw:space\": \"cosine\"}\n",
        "\n",
        "    # 개선된 청킹 설정\n",
        "    MIN_CHUNK_LENGTH: int = 50\n",
        "    MAX_CHUNK_LENGTH: int = 1500  # 법률 조항 최적화\n",
        "    OVERLAP_SIZE: int = 200 # 문장 분할 시 앞뒤 문맥을 더 많이 포함하도록 설정\n",
        "    MAX_SENTENCE_LENGTH: int = 1200\n",
        "    BATCH_SIZE: int = 100\n",
        "    MAX_WORD_SPLIT: int = 400\n",
        "\n",
        "    # 성능 설정\n",
        "    MAX_WORKERS: int = min(4, (os.cpu_count() or 1))\n",
        "    PROCESSING_TIMEOUT: int = 300  # 5분\n",
        "    ENABLE_PARALLEL: bool = True\n",
        "    CHUNK_CACHE_SIZE: int = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCTemIJSzDy4"
      },
      "outputs": [],
      "source": [
        "class LegalSentenceSplitter:\n",
        "    \"\"\"법률 문서 특화 문장 분할기\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # 법률 문서에서 마침표가 문장 끝이 아닌 경우들\n",
        "        self.abbreviation_patterns = [\n",
        "            r'\\b(?:제|항|호|목|조|장|절|편|부|별표|별지)\\s*\\d+(?:\\s*의\\s*\\d+)?\\s*\\.',\n",
        "            r'\\b\\d+\\.\\s*(?=\\d)',  # 번호 매김\n",
        "            r'\\b[A-Z]\\.',  # 단일 대문자 약어\n",
        "            r'(?:등|기타|포함|예시|단서|다만|그러나|다른|이외|기준|대상)\\.',\n",
        "            r'(?:법|시행령|시행규칙|고시|훈령|예규)\\.',\n",
        "        ]\n",
        "\n",
        "        # 실제 문장 끝을 나타내는 패턴\n",
        "        self.sentence_end_patterns = [\n",
        "            r'(?<=[다음과같습니다])\\.',\n",
        "            r'(?<=[이다])\\.',\n",
        "            r'(?<=[한다])\\.',\n",
        "            r'(?<=[된다])\\.',\n",
        "            r'(?<=[않는다])\\.',\n",
        "            r'(?<=[있다])\\.',\n",
        "            r'(?<=[없다])\\.',\n",
        "        ]\n",
        "\n",
        "        self.compiled_abbrev = [re.compile(p) for p in self.abbreviation_patterns]\n",
        "        self.compiled_sent_end = [re.compile(p) for p in self.sentence_end_patterns]\n",
        "\n",
        "    def split_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"법률 문서에 특화된 문장 분할\"\"\"\n",
        "        if not text.strip():\n",
        "            return []\n",
        "\n",
        "        # 임시 플레이스홀더로 약어의 마침표를 보호\n",
        "        protected_text = text\n",
        "        placeholders = {}\n",
        "\n",
        "        for i, pattern in enumerate(self.compiled_abbrev):\n",
        "            matches = list(pattern.finditer(protected_text))\n",
        "            for match in reversed(matches):  # 역순으로 처리하여 인덱스 보존\n",
        "                placeholder = f\"__ABBREV_{i}_{len(placeholders)}__\"\n",
        "                placeholders[placeholder] = match.group()\n",
        "                protected_text = protected_text[:match.start()] + placeholder + protected_text[match.end():]\n",
        "\n",
        "        # 문장 분할\n",
        "        sentences = []\n",
        "        current_sentence = \"\"\n",
        "\n",
        "        # 개선된 문장 분할 로직\n",
        "        parts = re.split(r'([.!?])\\s+', protected_text)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(parts):\n",
        "            current_sentence += parts[i]\n",
        "\n",
        "            if i + 1 < len(parts) and parts[i + 1] in '.!?':\n",
        "                current_sentence += parts[i + 1]\n",
        "\n",
        "                # 실제 문장 끝인지 확인\n",
        "                if self._is_sentence_end(current_sentence):\n",
        "                    sentences.append(current_sentence.strip())\n",
        "                    current_sentence = \"\"\n",
        "                else:\n",
        "                    current_sentence += \" \"  # 계속 이어짐\n",
        "\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        if current_sentence.strip():\n",
        "            sentences.append(current_sentence.strip())\n",
        "\n",
        "        # 플레이스홀더 복원\n",
        "        restored_sentences = []\n",
        "        for sentence in sentences:\n",
        "            restored = sentence\n",
        "            for placeholder, original in placeholders.items():\n",
        "                restored = restored.replace(placeholder, original)\n",
        "\n",
        "            if restored.strip():\n",
        "                restored_sentences.append(restored.strip())\n",
        "\n",
        "        return restored_sentences\n",
        "\n",
        "    def _is_sentence_end(self, text: str) -> bool:\n",
        "        \"\"\"실제 문장 끝인지 판단\"\"\"\n",
        "        # 문장 끝 패턴 매칭\n",
        "        for pattern in self.compiled_sent_end:\n",
        "            if pattern.search(text):\n",
        "                return True\n",
        "\n",
        "        # 추가 휴리스틱\n",
        "        text = text.strip()\n",
        "        if len(text) < 10:  # 너무 짧으면 문장이 아님\n",
        "            return False\n",
        "\n",
        "        # 법률 조문의 특징적 끝맺음\n",
        "        legal_endings = ['한다', '이다', '된다', '않는다', '있다', '없다', '같다', '바와 같다']\n",
        "        for ending in legal_endings:\n",
        "            if text.endswith(ending + '.'):\n",
        "                return True\n",
        "\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Sxk379zIwO"
      },
      "outputs": [],
      "source": [
        "class FileHashManager:\n",
        "    \"\"\"파일 해시 및 메타데이터 관리\"\"\"\n",
        "\n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self._init_db()\n",
        "\n",
        "    def _init_db(self):\n",
        "        \"\"\"메타데이터 DB 초기화\"\"\"\n",
        "        with sqlite3.connect(self.db_path) as conn:\n",
        "            conn.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS file_metadata (\n",
        "                    file_path TEXT PRIMARY KEY,\n",
        "                    file_hash TEXT NOT NULL,\n",
        "                    last_modified REAL NOT NULL,\n",
        "                    chunk_count INTEGER DEFAULT 0,\n",
        "                    processing_time REAL DEFAULT 0,\n",
        "                    last_processed TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "            conn.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS chunk_metadata (\n",
        "                    chunk_id TEXT PRIMARY KEY,\n",
        "                    parent_chunk_id TEXT,\n",
        "                    file_path TEXT NOT NULL,\n",
        "                    chunk_index INTEGER NOT NULL,\n",
        "                    is_sub_chunk BOOLEAN DEFAULT FALSE,\n",
        "                    creation_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                    FOREIGN KEY (file_path) REFERENCES file_metadata (file_path)\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def get_file_hash(self, file_path: str) -> str:\n",
        "        \"\"\"파일 해시 계산\"\"\"\n",
        "        hash_sha256 = hashlib.sha256()\n",
        "        try:\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "                    hash_sha256.update(chunk)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"파일 해시 계산 실패 {file_path}: {e}\")\n",
        "            # 파일 수정시간과 크기를 기반으로 폴백 해시\n",
        "            stat = os.stat(file_path)\n",
        "            fallback_string = f\"{file_path}_{stat.st_mtime}_{stat.st_size}\"\n",
        "            return hashlib.sha256(fallback_string.encode()).hexdigest()\n",
        "\n",
        "        return hash_sha256.hexdigest()\n",
        "\n",
        "    def should_process_file(self, file_path: str) -> bool:\n",
        "        \"\"\"파일 처리 필요 여부 확인\"\"\"\n",
        "        try:\n",
        "            current_hash = self.get_file_hash(file_path)\n",
        "            current_mtime = os.path.getmtime(file_path)\n",
        "\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                cursor = conn.execute(\n",
        "                    'SELECT file_hash, last_modified FROM file_metadata WHERE file_path = ?',\n",
        "                    (file_path,)\n",
        "                )\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if result is None:\n",
        "                    return True  # 새 파일\n",
        "\n",
        "                stored_hash, stored_mtime = result\n",
        "                return current_hash != stored_hash or abs(current_mtime - stored_mtime) > 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"파일 처리 필요성 확인 실패 {file_path}: {e}\")\n",
        "            return True  # 오류 시 안전하게 처리\n",
        "\n",
        "    def update_file_metadata(self, file_path: str, chunk_count: int, processing_time: float):\n",
        "        \"\"\"파일 메타데이터 업데이트\"\"\"\n",
        "        try:\n",
        "            file_hash = self.get_file_hash(file_path)\n",
        "            mtime = os.path.getmtime(file_path)\n",
        "\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                conn.execute('''\n",
        "                    INSERT OR REPLACE INTO file_metadata\n",
        "                    (file_path, file_hash, last_modified, chunk_count, processing_time)\n",
        "                    VALUES (?, ?, ?, ?, ?)\n",
        "                ''', (file_path, file_hash, mtime, chunk_count, processing_time))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"파일 메타데이터 업데이트 실패 {file_path}: {e}\")\n",
        "\n",
        "    def register_chunks(self, file_path: str, chunk_ids: List[str], parent_chunk_map: Dict[str, str]):\n",
        "        \"\"\"청크 메타데이터 등록\"\"\"\n",
        "        try:\n",
        "            with sqlite3.connect(self.db_path) as conn:\n",
        "                # 기존 청크 삭제\n",
        "                conn.execute('DELETE FROM chunk_metadata WHERE file_path = ?', (file_path,))\n",
        "\n",
        "                # 새 청크 등록\n",
        "                for i, chunk_id in enumerate(chunk_ids):\n",
        "                    parent_id = parent_chunk_map.get(chunk_id)\n",
        "                    is_sub_chunk = parent_id is not None\n",
        "\n",
        "                    conn.execute('''\n",
        "                        INSERT INTO chunk_metadata\n",
        "                        (chunk_id, parent_chunk_id, file_path, chunk_index, is_sub_chunk)\n",
        "                        VALUES (?, ?, ?, ?, ?)\n",
        "                    ''', (chunk_id, parent_id, file_path, i, is_sub_chunk))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"청크 메타데이터 등록 실패 {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he_4n-Z5zM02"
      },
      "outputs": [],
      "source": [
        "class EnhancedLegalParser:\n",
        "    \"\"\"개선된 법률 문서 구조 파서\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # 기존 패턴 유지하되 개선\n",
        "        self.patterns = {\n",
        "            'chapter': re.compile(r\"^\\s*제\\s*([0-9]+(?:의[0-9]+)?)\\s*장\\s*(.*)$\", re.MULTILINE),\n",
        "            'section': re.compile(r\"^\\s*제\\s*([0-9]+(?:의[0-9]+)?)\\s*절\\s*(.*)$\", re.MULTILINE),\n",
        "            'article': re.compile(r\"^\\s*제\\s*([0-9]+(?:의[0-9]+)?)\\s*조\\s*(?:\\(([^)]+)\\))?\\s*(.*)$\", re.MULTILINE),\n",
        "            'paragraph': re.compile(r\"^\\s*(①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩|⑪|⑫|⑬|⑭|⑮|⑯|⑰|⑱|⑲|⑳)\\s*(.*)$\", re.MULTILINE),\n",
        "            'item': re.compile(r\"^\\s*([0-9]+)\\.\\s*(.*)$\", re.MULTILINE),\n",
        "            'subitem': re.compile(r\"^\\s*([가-힣])\\.\\s*(.*)$\", re.MULTILINE),\n",
        "            'appendix': re.compile(r\"^\\s*부\\s*칙\\s*(.*)$\", re.MULTILINE),\n",
        "            'attachment': re.compile(r\"^\\s*(\\[별표\\s*[0-9]+.*?\\]|별지\\s*제[0-9]+호서식)\\s*(.*)$\", re.MULTILINE),\n",
        "        }\n",
        "\n",
        "        self.cross_ref_patterns = {\n",
        "            'article_ref': re.compile(r\"제\\s*([0-9]+(?:의[0-9]+)?)\\s*조\"),\n",
        "            'paragraph_ref': re.compile(r\"제\\s*([0-9]+)\\s*항\"),\n",
        "            'item_ref': re.compile(r\"제\\s*([0-9]+)\\s*호\"),\n",
        "            'law_ref': re.compile(r\"「([^」]+)」\"),\n",
        "        }\n",
        "\n",
        "        self.paragraph_map = {\n",
        "            '①': '1', '②': '2', '③': '3', '④': '4', '⑤': '5',\n",
        "            '⑥': '6', '⑦': '7', '⑧': '8', '⑨': '9', '⑩': '10',\n",
        "            '⑪': '11', '⑫': '12', '⑬': '13', '⑭': '14', '⑮': '15',\n",
        "            '⑯': '16', '⑰': '17', '⑱': '18', '⑲': '19', '⑳': '20'\n",
        "        }\n",
        "\n",
        "        # 시간 정보 추출을 위한 정규식\n",
        "        self.temporal_patterns = {\n",
        "            # '[시행 2025. 4. 22.]' 형식과 '시행일: 2025. 4. 22.' 형식 모두 처리\n",
        "            \"effective_date\": re.compile(r\"\\[?\\s*시행(?:일)?\\s*:?\\s*(\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\.?\\s*\\]?\"),\n",
        "\n",
        "            # '[법률 제...호, 2025. 4. 22., ...]' 형식에서 공포일 추출\n",
        "            \"publication_date\": re.compile(r\"법률\\s*제[0-9]+호\\s*,\\s*(\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\.?\"),\n",
        "\n",
        "            # 법률 번호, 날짜, 개정 종류를 포함하여 추출\n",
        "            \"amendment_info\": re.compile(r\"(법률\\s*제[0-9]+호,\\s*\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2}\\.?,\\s*.*?개정)\"),\n",
        "        }\n",
        "\n",
        "    def parse_structure(self, text: str) -> Optional[LegalStructure]:\n",
        "        \"\"\"텍스트에서 법률 구조 정보를 추출\"\"\"\n",
        "        text = text.strip()\n",
        "        if not text:\n",
        "            return None\n",
        "\n",
        "        for structure_type, pattern in self.patterns.items():\n",
        "            match = pattern.match(text)\n",
        "            if match:\n",
        "                return self._create_structure(structure_type, match, text)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _create_structure(self, structure_type: str, match, original_text: str) -> LegalStructure:\n",
        "        \"\"\"매치 결과로부터 LegalStructure 객체 생성\"\"\"\n",
        "        level_map = {\n",
        "            'chapter': 1, 'section': 2, 'article': 3,\n",
        "            'paragraph': 4, 'item': 5, 'subitem': 6,\n",
        "            'appendix': 1, 'attachment': 2\n",
        "        }\n",
        "\n",
        "        if structure_type == 'paragraph':\n",
        "            number = self.paragraph_map.get(match.group(1), match.group(1))\n",
        "            title = match.group(2).strip() if match.group(2) else \"\"\n",
        "        elif structure_type in ['chapter', 'section', 'article']:\n",
        "            number = match.group(1)\n",
        "            if structure_type == 'article' and match.lastindex >= 3:\n",
        "                subtitle = match.group(2) if match.group(2) else \"\"\n",
        "                content = match.group(3) if match.group(3) else \"\"\n",
        "                title = f\"{subtitle} {content}\".strip() if subtitle else content.strip()\n",
        "            else:\n",
        "                title = match.group(2).strip() if match.lastindex >= 2 else \"\"\n",
        "        elif structure_type in ['item', 'subitem']:\n",
        "            number = match.group(1)\n",
        "            title = match.group(2).strip() if match.group(2) else \"\"\n",
        "        else:\n",
        "            number = \"1\"\n",
        "            title = match.group(1).strip() if match.group(1) else \"\"\n",
        "\n",
        "        cross_refs = self._extract_cross_references(original_text)\n",
        "\n",
        "        return LegalStructure(\n",
        "            structure_type=structure_type,\n",
        "            number=number,\n",
        "            title=title,\n",
        "            level=level_map.get(structure_type, 7),\n",
        "            cross_refs=cross_refs\n",
        "        )\n",
        "\n",
        "    def _extract_cross_references(self, text: str) -> List[str]:\n",
        "        \"\"\"텍스트에서 상호 참조 추출\"\"\"\n",
        "        refs = []\n",
        "        for ref_type, pattern in self.cross_ref_patterns.items():\n",
        "            matches = pattern.findall(text)\n",
        "            for match in matches:\n",
        "                if ref_type == 'law_ref':\n",
        "                    refs.append(f\"법률:{match}\")\n",
        "                elif ref_type == 'article_ref':\n",
        "                    refs.append(f\"조:{match}\")\n",
        "                elif ref_type == 'paragraph_ref':\n",
        "                    refs.append(f\"항:{match}\")\n",
        "                elif ref_type == 'item_ref':\n",
        "                    refs.append(f\"호:{match}\")\n",
        "        return list(set(refs))\n",
        "\n",
        "    def extract_temporal_info(self, text: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"텍스트에서 시간 관련 메타데이터 추출\"\"\"\n",
        "        info = {\n",
        "            \"effective_date\": None,\n",
        "            \"publication_date\": None,\n",
        "            \"amendment_info\": None,\n",
        "        }\n",
        "        for key, pattern in self.temporal_patterns.items():\n",
        "            match = pattern.search(text)\n",
        "            if match:\n",
        "                info[key] = match.group(1).strip()\n",
        "        return info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsHh4VQSzRBF"
      },
      "outputs": [],
      "source": [
        "class EnhancedDocumentProcessor:\n",
        "    \"\"\"개선된 문서 처리기\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str, hash_manager: FileHashManager):\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"지정된 파일을 찾을 수 없습니다: {file_path}\")\n",
        "\n",
        "        self.file_path = file_path\n",
        "        self.file_name = os.path.basename(file_path)\n",
        "        self.parser = EnhancedLegalParser()\n",
        "        self.sentence_splitter = LegalSentenceSplitter()\n",
        "        self.hash_manager = hash_manager\n",
        "        self.structure_hierarchy = []\n",
        "        self.temporal_metadata = {}\n",
        "\n",
        "        self.chunk_id_counter = 0\n",
        "        self.parent_chunk_map = {}\n",
        "\n",
        "    # 테이블을 Markdown으로 변환하는 헬퍼 함수\n",
        "    def _convert_docx_table_to_markdown(self, table: DocxTable) -> str:\n",
        "        \"\"\"Docx 테이블을 Markdown 형식으로 변환\"\"\"\n",
        "        markdown_text = \"\\n\\n| \"\n",
        "        try:\n",
        "            header_cells = [cell.text.strip().replace('\\n', ' ') for cell in table.rows[0].cells]\n",
        "            markdown_text += \" | \".join(header_cells) + \" |\\n\"\n",
        "            markdown_text += \"| \" + \" | \".join([\"---\"] * len(header_cells)) + \" |\\n\"\n",
        "\n",
        "            for row in table.rows[1:]:\n",
        "                row_cells = [cell.text.strip().replace('\\n', ' ') for cell in row.cells]\n",
        "                markdown_text += \"| \" + \" | \".join(row_cells) + \" |\\n\"\n",
        "        except IndexError:\n",
        "            logger.warning(f\"'{self.file_name}'에서 비정상적인 테이블 구조 발견, 건너뜁니다.\")\n",
        "            return \"\"\n",
        "        return markdown_text.strip() + \"\\n\\n\"\n",
        "\n",
        "    def _convert_pptx_table_to_markdown(self, table: PptxTable) -> str:\n",
        "        \"\"\"PPTX 테이블을 Markdown 형식으로 변환 (수정된 버전)\"\"\"\n",
        "        markdown_text = \"\\n\\n| \"\n",
        "        try:\n",
        "            # 테이블의 컬럼 수를 기준으로 순회하도록 수정\n",
        "            num_cols = len(table.columns)\n",
        "            header_cells = [table.cell(0, c).text.strip().replace('\\n', ' ') for c in range(num_cols)]\n",
        "            markdown_text += \" | \".join(header_cells) + \" |\\n\"\n",
        "            markdown_text += \"| \" + \" | \".join([\"---\"] * len(header_cells)) + \" |\\n\"\n",
        "\n",
        "            # row와 col 인덱스를 사용하여 셀 텍스트에 접근\n",
        "            for r in range(1, len(table.rows)):\n",
        "                row_cells = [table.cell(r, c).text.strip().replace('\\n', ' ') for c in range(num_cols)]\n",
        "                markdown_text += \"| \" + \" | \".join(row_cells) + \" |\\n\"\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"'{self.file_name}'의 PPTX 테이블 변환 중 오류: {e}\")\n",
        "            return \"\"\n",
        "        return markdown_text.strip() + \"\\n\\n\"\n",
        "\n",
        "    def _extract_initial_metadata_from_text(self, text: str):\n",
        "        \"\"\"일반 텍스트에서 시간 메타데이터 추출\"\"\"\n",
        "        # 텍스트의 앞부분 30줄을 검사\n",
        "        initial_lines = text.split('\\n')[:30]\n",
        "        temp_info = {k: None for k in self.parser.temporal_patterns.keys()}\n",
        "\n",
        "        for line in initial_lines:\n",
        "            if all(temp_info.values()): break\n",
        "            extracted = self.parser.extract_temporal_info(line)\n",
        "            for key, value in extracted.items():\n",
        "                if not temp_info.get(key) and value:\n",
        "                    temp_info[key] = value\n",
        "        self.temporal_metadata = temp_info\n",
        "    def _get_text_blocks(self) -> List[str]:\n",
        "        \"\"\"파일 형식에 따라 텍스트 블록 리스트를 추출\"\"\"\n",
        "        file_ext = Path(self.file_path).suffix.lower()\n",
        "        blocks = []\n",
        "\n",
        "        if file_ext == '.docx':\n",
        "            document = docx.Document(self.file_path)\n",
        "            # DOCX는 자체 단락 구조에서 메타데이터를 찾는 것이 더 정확\n",
        "            initial_paragraphs = [p.text for p in document.paragraphs[:15]]\n",
        "            self._extract_initial_metadata_from_text(\"\\n\".join(initial_paragraphs))\n",
        "            for element in document.element.body:\n",
        "                if isinstance(element, CT_P):\n",
        "                    blocks.append(Paragraph(element, document).text.strip())\n",
        "                elif isinstance(element, CT_Tbl):\n",
        "                    blocks.append(self._convert_docx_table_to_markdown(DocxTable(element, document)))\n",
        "\n",
        "        elif file_ext == '.pdf':\n",
        "            full_text = \"\"\n",
        "            with fitz.open(self.file_path) as doc:\n",
        "                for page in doc:\n",
        "                    full_text += page.get_text(sort=True) + \"\\n\"\n",
        "            self._extract_initial_metadata_from_text(full_text)\n",
        "            blocks = full_text.split('\\n')\n",
        "\n",
        "        elif file_ext == '.pptx':\n",
        "            prs = Presentation(self.file_path)\n",
        "            full_text_for_metadata = \"\"\n",
        "            for slide in prs.slides:\n",
        "                for shape in slide.shapes:\n",
        "                    if shape.has_table:\n",
        "                        table_text = self._convert_pptx_table_to_markdown(shape.table)\n",
        "                        blocks.append(table_text)\n",
        "                        full_text_for_metadata += table_text + \"\\n\"\n",
        "                    elif shape.has_text_frame:\n",
        "                        text = shape.text_frame.text.strip()\n",
        "                        blocks.append(text)\n",
        "                        full_text_for_metadata += text + \"\\n\"\n",
        "            self._extract_initial_metadata_from_text(full_text_for_metadata)\n",
        "\n",
        "        else:\n",
        "            logger.warning(f\"지원하지 않는 파일 형식입니다: {self.file_name}\")\n",
        "\n",
        "        return blocks\n",
        "\n",
        "\n",
        "    def extract_structured_chunks(self) -> Tuple[List[Dict[str, Any]], Dict[str, str]]:\n",
        "        \"\"\"모든 파일 형식에 대해 구조화된 텍스트 청크를 추출하는 범용 메서드\"\"\"\n",
        "        try:\n",
        "            text_blocks = self._get_text_blocks()\n",
        "            if not text_blocks:\n",
        "                logger.warning(f\"'{self.file_name}'에서 텍스트를 추출하지 못했습니다.\")\n",
        "                return [], {}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"'{self.file_name}' 파일 읽기 또는 파싱 실패: {e}\", exc_info=True)\n",
        "            return [], {}\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = None\n",
        "        file_hash = self.hash_manager.get_file_hash(self.file_path)\n",
        "        print(f\"    -> '{self.file_name}': 청크 추출 및 분석 시작...\")\n",
        "\n",
        "        for text in text_blocks:\n",
        "            text = text.strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            structure = self.parser.parse_structure(text)\n",
        "            if structure:\n",
        "                if current_chunk and current_chunk['content']:\n",
        "                    chunks.append(self._finalize_chunk(current_chunk, file_hash))\n",
        "\n",
        "                self._update_hierarchy(structure)\n",
        "                chunk_id = self._generate_chunk_id()\n",
        "                structure.original_chunk_id = chunk_id\n",
        "                current_chunk = {\n",
        "                    'chunk_id': chunk_id, 'structure': structure, 'content': [text],\n",
        "                    'hierarchy_path': self._get_hierarchy_path(),\n",
        "                    'parent_structures': self._get_parent_structures()\n",
        "                }\n",
        "            else:\n",
        "                if not current_chunk: # 문서 시작 부분\n",
        "                    chunk_id = self._generate_chunk_id()\n",
        "                    current_chunk = {\n",
        "                        'chunk_id': chunk_id,\n",
        "                        'structure': LegalStructure('preamble', '0', '서문', 0, original_chunk_id=chunk_id),\n",
        "                        'content': [text], 'hierarchy_path': [], 'parent_structures': []\n",
        "                    }\n",
        "                else:\n",
        "                    current_chunk['content'].append(text)\n",
        "\n",
        "        if current_chunk and current_chunk['content']:\n",
        "            chunks.append(self._finalize_chunk(current_chunk, file_hash))\n",
        "\n",
        "        processed_chunks = self._post_process_chunks_advanced(chunks)\n",
        "        logger.info(f\"'{self.file_name}'에서 {len(processed_chunks)}개의 청크를 추출했습니다.\")\n",
        "        gc.collect()\n",
        "        print(f\"    -> '{self.file_name}': 청크 {len(processed_chunks)}개 추출 완료.\")\n",
        "        return processed_chunks, self.parent_chunk_map\n",
        "\n",
        "    def _generate_chunk_id(self) -> str:\n",
        "        \"\"\"고유 청크 ID 생성\"\"\"\n",
        "        self.chunk_id_counter += 1\n",
        "        timestamp = str(int(time.time()))\n",
        "        return f\"{self.file_name}_{self.chunk_id_counter:04d}_{timestamp}\"\n",
        "\n",
        "    def _update_hierarchy(self, structure: LegalStructure):\n",
        "        \"\"\"계층 구조 업데이트\"\"\"\n",
        "        self.structure_hierarchy = [\n",
        "            s for s in self.structure_hierarchy if s.level < structure.level\n",
        "        ]\n",
        "\n",
        "        if self.structure_hierarchy:\n",
        "            structure.parent_ref = f\"{self.structure_hierarchy[-1].structure_type}:{self.structure_hierarchy[-1].number}\"\n",
        "\n",
        "        self.structure_hierarchy.append(structure)\n",
        "\n",
        "    def _get_hierarchy_path(self) -> List[str]:\n",
        "        \"\"\"현재 계층 경로 반환\"\"\"\n",
        "        return [f\"{s.structure_type}:{s.number}\" for s in self.structure_hierarchy]\n",
        "\n",
        "    def _get_parent_structures(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"상위 구조 정보 반환\"\"\"\n",
        "        return [\n",
        "            {\n",
        "                'type': s.structure_type,\n",
        "                'number': s.number,\n",
        "                'title': s.title,\n",
        "                'level': s.level\n",
        "            }\n",
        "            for s in self.structure_hierarchy[:-1]\n",
        "        ]\n",
        "\n",
        "    def _finalize_chunk(self, chunk: Dict[str, Any], file_hash: str) -> Dict[str, Any]:\n",
        "        \"\"\"청크를 최종 형태로 변환\"\"\"\n",
        "        content = \"\\n\".join(chunk['content'])\n",
        "        structure = chunk['structure']\n",
        "\n",
        "        # 청크의 제목을 생성\n",
        "        chunk_title = f\"{structure.structure_type}:{structure.number} {structure.title}\".strip()\n",
        "\n",
        "        # 청크 내용 맨 앞에 제목을 붙여 문맥 정보를 강화합니다.\n",
        "        final_content = f\"[{chunk_title}]\\\\n{content}\"\n",
        "\n",
        "        final_chunk = {\n",
        "            'chunk_id': chunk['chunk_id'],\n",
        "            'title': chunk_title,\n",
        "            'content': final_content,\n",
        "            'structure_info': {\n",
        "                'type': structure.structure_type,\n",
        "                'number': structure.number,\n",
        "                'level': structure.level,\n",
        "                'parent_ref': structure.parent_ref,\n",
        "                'cross_refs': structure.cross_refs,\n",
        "                'original_chunk_id': structure.original_chunk_id\n",
        "            },\n",
        "            'hierarchy_path': chunk['hierarchy_path'],\n",
        "            'parent_structures': chunk['parent_structures'],\n",
        "            'char_count': len(content),\n",
        "            'word_count': len(content.split()),\n",
        "            'file_hash': file_hash\n",
        "        }\n",
        "\n",
        "        # 추출된 시간 메타데이터를 청크에 추가\n",
        "        final_chunk.update(self.temporal_metadata)\n",
        "\n",
        "        return final_chunk\n",
        "\n",
        "    def _post_process_chunks_advanced(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"향상된 청크 후처리\"\"\"\n",
        "        processed = []\n",
        "        config = Config()\n",
        "\n",
        "        for chunk in chunks:\n",
        "            if chunk['char_count'] < config.MIN_CHUNK_LENGTH:\n",
        "                # if chunk['structure_info']['type'] in ['article', 'chapter', 'section']:\n",
        "                #     processed.append(chunk)\n",
        "                continue\n",
        "\n",
        "            if chunk['char_count'] > config.MAX_CHUNK_LENGTH:\n",
        "                # 청킹 시 문맥 유지를 위해 오버랩 개념을 명시적으로 고려할 수 있음\n",
        "                # 현재는 문장 단위 분할이 우선되지만, 향후 슬라이딩 윈도우 적용 시 이 부분 수정\n",
        "                split_chunks = self._split_chunk_advanced(chunk, config.MAX_CHUNK_LENGTH)\n",
        "                processed.extend(split_chunks)\n",
        "            else:\n",
        "                processed.append(chunk)\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def _split_chunk_advanced(self, chunk: Dict[str, Any], max_length: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"향상된 청크 분할\"\"\"\n",
        "        content = chunk['content']\n",
        "        structure_info = chunk['structure_info']\n",
        "\n",
        "        # 법률 구조별 분할 전략\n",
        "        if structure_info['type'] == 'article':\n",
        "            return self._split_by_paragraphs_advanced(chunk, max_length)\n",
        "        elif structure_info['type'] == 'paragraph':\n",
        "            return self._split_by_items_advanced(chunk, max_length)\n",
        "        else:\n",
        "            return self._split_by_sentences_advanced(chunk, max_length)\n",
        "\n",
        "    def _split_by_sentences_advanced(self, chunk: Dict[str, Any], max_length: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"향상된 문장 단위 분할\"\"\"\n",
        "        content = chunk['content']\n",
        "        sentences = self.sentence_splitter.split_sentences(content)\n",
        "\n",
        "        if len(sentences) <= 1:\n",
        "            # 단일 문장이 너무 긴 경우 - 의미 단위로 분할\n",
        "            return self._split_single_long_sentence(chunk, max_length)\n",
        "\n",
        "        split_chunks = []\n",
        "        current_content = []\n",
        "        current_length = 0\n",
        "\n",
        "        # 문장 윈도우 전략과 유사한 접근\n",
        "        # N개의 문장을 하나의 청크로 구성하고, 길이가 초과되면 분할\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_length = len(sentence)\n",
        "\n",
        "            if current_length + sentence_length > max_length and current_content:\n",
        "                # 현재 청크 저장\n",
        "                sub_chunk_text = ' '.join(current_content)\n",
        "                sub_chunk = self._create_sub_chunk(chunk, sub_chunk_text, len(split_chunks) + 1)\n",
        "                split_chunks.append(sub_chunk)\n",
        "\n",
        "                # 슬라이딩 윈도우(오버랩) 구현\n",
        "                # 이전 청크의 마지막 문장을 현재 청크의 시작 부분에 포함\n",
        "                overlap_sentences = [s for s in current_content[-2:] if len(' '.join(current_content[-2:])) < Config.OVERLAP_SIZE]\n",
        "                current_content = overlap_sentences + [sentence]\n",
        "                current_length = len(' '.join(current_content))\n",
        "            else:\n",
        "                current_content.append(sentence)\n",
        "                current_length += sentence_length\n",
        "\n",
        "        if current_content:\n",
        "            sub_chunk = self._create_sub_chunk(chunk, ' '.join(current_content), len(split_chunks) + 1)\n",
        "            split_chunks.append(sub_chunk)\n",
        "\n",
        "        return split_chunks if split_chunks else [chunk]\n",
        "\n",
        "    def _split_single_long_sentence(self, chunk: Dict[str, Any], max_length: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"단일 긴 문장을 의미 단위로 분할\"\"\"\n",
        "        content = chunk['content']\n",
        "\n",
        "        # 의미 기반 청킹을 위한 분할점 (정규식 기반)\n",
        "        split_points = [\n",
        "            r'(?<=다만)\\s*,?\\s*',    # 단서 조항\n",
        "            r'(?<=그러나)\\s*,?\\s*',  # 예외 조항\n",
        "            r'(?<=다음과\\s같다)\\s*:?\\s*',  # 열거 시작\n",
        "            r'(다음\\\\s각\\\\s호(?:의 어느 하나에 해당하는 경우)?\\\\s*:?\\\\s*)',  # 각호 시작\n",
        "            r'(?<=이\\s경우)\\s*,?\\s*',     # 조건 시작\n",
        "            r'(?<=[.\\)\\]])\\s+(?=한편|또한|그리고)', # 접속 부사\n",
        "            r';\\s+', # 세미콜론은 문맥상 중요한 분리점일 수 있음\n",
        "        ]\n",
        "\n",
        "        # 의미 단위로 분할 시도\n",
        "        parts = [content]\n",
        "        for pattern in split_points:\n",
        "            new_parts = []\n",
        "            for part in parts:\n",
        "                if len(part) > max_length:\n",
        "                    # re.split은 구분자도 결과에 포함시키기 위해 괄호로 묶음\n",
        "                    split_result = re.split(f'({pattern})', part)\n",
        "                    # 분리된 텍스트와 구분자를 다시 합쳐서 의미 유지\n",
        "                    temp_part = \"\"\n",
        "                    for j in range(0, len(split_result), 2):\n",
        "                        segment = split_result[j]\n",
        "                        delimiter = split_result[j+1] if j+1 < len(split_result) else \"\"\n",
        "                        if len(temp_part) + len(segment) + len(delimiter) > max_length and temp_part:\n",
        "                             new_parts.append(temp_part)\n",
        "                             temp_part = segment + delimiter\n",
        "                        else:\n",
        "                             temp_part += segment + delimiter\n",
        "                    if temp_part:\n",
        "                        new_parts.append(temp_part)\n",
        "                else:\n",
        "                    new_parts.append(part)\n",
        "            parts = new_parts\n",
        "\n",
        "        # 여전히 긴 부분이 있으면 단어 단위로 분할\n",
        "        final_parts = []\n",
        "        for part in parts:\n",
        "            if len(part) > max_length:\n",
        "                words = part.split()\n",
        "                current_part = []\n",
        "                current_length = 0\n",
        "\n",
        "                for word in words:\n",
        "                    word_length = len(word) + 1  # 공백 포함\n",
        "                    if current_length + word_length > max_length and current_part:\n",
        "                        final_parts.append(' '.join(current_part))\n",
        "                        current_part = [word]\n",
        "                        current_length = word_length\n",
        "                    else:\n",
        "                        current_part.append(word)\n",
        "                        current_length += word_length\n",
        "\n",
        "                if current_part:\n",
        "                    final_parts.append(' '.join(current_part))\n",
        "            else:\n",
        "                final_parts.append(part)\n",
        "\n",
        "        # 청크 생성\n",
        "        split_chunks = []\n",
        "        for i, part in enumerate(final_parts):\n",
        "            if part.strip():\n",
        "                sub_chunk = self._create_sub_chunk(chunk, part.strip(), i + 1)\n",
        "                split_chunks.append(sub_chunk)\n",
        "\n",
        "        return split_chunks if split_chunks else [chunk]\n",
        "\n",
        "    def _split_by_paragraphs_advanced(self, chunk: Dict[str, Any], max_length: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"향상된 항 단위 분할\"\"\"\n",
        "        content = chunk['content']\n",
        "        paragraph_pattern = re.compile(r'(①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩|⑪|⑫|⑬|⑭|⑮|⑯|⑰|⑱|⑲|⑳)')\n",
        "\n",
        "        paragraphs = paragraph_pattern.split(content)\n",
        "        if len(paragraphs) <= 1:\n",
        "            return self._split_by_sentences_advanced(chunk, max_length)\n",
        "\n",
        "        split_chunks = []\n",
        "        current_content = paragraphs[0] if paragraphs[0].strip() else \"\"\n",
        "\n",
        "        i = 1\n",
        "        while i < len(paragraphs):\n",
        "            if i + 1 < len(paragraphs):\n",
        "                paragraph_marker = paragraphs[i]\n",
        "                paragraph_content = paragraphs[i + 1]\n",
        "                paragraph_text = paragraph_marker + paragraph_content\n",
        "\n",
        "                if len(current_content) + len(paragraph_text) > max_length and current_content.strip():\n",
        "                    sub_chunk = self._create_sub_chunk(chunk, current_content, len(split_chunks) + 1)\n",
        "                    split_chunks.append(sub_chunk)\n",
        "                    current_content = paragraph_text\n",
        "                else:\n",
        "                    current_content += paragraph_text\n",
        "\n",
        "                i += 2\n",
        "            else:\n",
        "                current_content += paragraphs[i]\n",
        "                i += 1\n",
        "\n",
        "        if current_content.strip():\n",
        "            sub_chunk = self._create_sub_chunk(chunk, current_content, len(split_chunks) + 1)\n",
        "            split_chunks.append(sub_chunk)\n",
        "\n",
        "        return split_chunks if split_chunks else [chunk]\n",
        "\n",
        "    def _split_by_items_advanced(self, chunk: Dict[str, Any], max_length: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"향상된 호 단위 분할\"\"\"\n",
        "        content = chunk['content']\n",
        "        item_pattern = re.compile(r'(\\d+\\.)')\n",
        "\n",
        "        items = item_pattern.split(content)\n",
        "        if len(items) <= 1:\n",
        "            return self._split_by_sentences_advanced(chunk, max_length)\n",
        "\n",
        "        split_chunks = []\n",
        "        current_content = items[0] if items[0].strip() else \"\"\n",
        "\n",
        "        i = 1\n",
        "        while i < len(items):\n",
        "            if i + 1 < len(items):\n",
        "                item_marker = items[i]\n",
        "                item_content = items[i + 1]\n",
        "                item_text = item_marker + item_content\n",
        "\n",
        "                if len(current_content) + len(item_text) > max_length and current_content.strip():\n",
        "                    sub_chunk = self._create_sub_chunk(chunk, current_content, len(split_chunks) + 1)\n",
        "                    split_chunks.append(sub_chunk)\n",
        "                    current_content = item_text\n",
        "                else:\n",
        "                    current_content += item_text\n",
        "\n",
        "                i += 2\n",
        "            else:\n",
        "                current_content += items[i]\n",
        "                i += 1\n",
        "\n",
        "        if current_content.strip():\n",
        "            sub_chunk = self._create_sub_chunk(chunk, current_content, len(split_chunks) + 1)\n",
        "            split_chunks.append(sub_chunk)\n",
        "\n",
        "        return split_chunks if split_chunks else [chunk]\n",
        "\n",
        "    def _create_sub_chunk(self, original_chunk: Dict[str, Any], content: str, sub_index: int) -> Dict[str, Any]:\n",
        "        \"\"\"향상된 하위 청크 생성\"\"\"\n",
        "        sub_chunk_id = f\"{original_chunk['chunk_id']}_sub_{sub_index}\"\n",
        "        parent_chunk_id = original_chunk['chunk_id']\n",
        "\n",
        "        # 부모-자식 관계 추적\n",
        "        self.parent_chunk_map[sub_chunk_id] = parent_chunk_id\n",
        "\n",
        "        new_chunk = original_chunk.copy()\n",
        "\n",
        "        contextual_title = original_chunk.get('title', '관련 조항')\n",
        "        final_content = f\"[{contextual_title} (부분)]\\\\n{content}\"\n",
        "\n",
        "        new_chunk.update({\n",
        "            'chunk_id': sub_chunk_id,\n",
        "            'title': f\"{original_chunk['title']} (부분 {sub_index})\",\n",
        "            'content': final_content,\n",
        "            'char_count': len(content),\n",
        "            'word_count': len(content.split()),\n",
        "            'is_sub_chunk': True,\n",
        "            'sub_index': sub_index,\n",
        "            'parent_chunk_id': parent_chunk_id\n",
        "        })\n",
        "        return new_chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFj_AzczikF"
      },
      "outputs": [],
      "source": [
        "class EnhancedVectorDBManager:\n",
        "    \"\"\"개선된 벡터 DB 관리자\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.hash_manager = FileHashManager(config.METADATA_DB_PATH)\n",
        "\n",
        "        logger.info(\"개선된 임베딩 모델을 로드하는 중입니다...\")\n",
        "        try:\n",
        "            # 법률 특화 모델 우선 시도\n",
        "            self.primary_embedding_func = self._load_best_embedding_model(config)\n",
        "\n",
        "            self.client = chromadb.PersistentClient(path=config.DB_PATH)\n",
        "            self._initialize_collection_safe(config.COLLECTION_NAME, config.METADATA)\n",
        "\n",
        "            logger.info(\"개선된 Vector DB 초기화가 완료되었습니다.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Vector DB 초기화 실패: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_best_embedding_model(self, config: Config):\n",
        "        logger.info(\"최적 임베딩 모델 선택을 시작합니다. (현재: 가용성 기반 선택)\")\n",
        "        logger.info(\"권장 사항: 프로덕션 환경에서는 Q&A 평가셋 기반의 실증적 성능 측정(MRR, Hit Rate)을 통해 모델을 선택해야 합니다.\")\n",
        "\n",
        "        models_to_try = config.EMBEDDING_MODELS\n",
        "        #  models_to_try = [config.LEGAL_SPECIALIZED_MODEL] + config.EMBEDDING_MODELS\n",
        "\n",
        "        for model in models_to_try:\n",
        "            try:\n",
        "                embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "                    model_name=model\n",
        "                )\n",
        "                # 테스트 임베딩으로 모델 검증\n",
        "                embedding_func([\"테스트\"])\n",
        "                logger.info(f\"임베딩 모델 로드 성공: {model}\")\n",
        "                return embedding_func\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"모델 {model} 로드 실패: {e}\")\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(\"사용 가능한 임베딩 모델이 없습니다.\")\n",
        "\n",
        "    def _initialize_collection_safe(self, collection_name: str, metadata: Dict[str, str]):\n",
        "        \"\"\"안전한 컬렉션 초기화 - 데이터 보존\"\"\"\n",
        "        try:\n",
        "            existing_collections = [col.name for col in self.client.list_collections()]\n",
        "\n",
        "            if collection_name in existing_collections:\n",
        "                # 기존 컬렉션 사용\n",
        "                self.collection = self.client.get_collection(\n",
        "                    name=collection_name,\n",
        "                    embedding_function=self.primary_embedding_func\n",
        "                )\n",
        "                logger.info(f\"기존 컬렉션 '{collection_name}'을 사용합니다.\")\n",
        "            else:\n",
        "                # 새 컬렉션 생성\n",
        "                self.collection = self.client.create_collection(\n",
        "                    name=collection_name,\n",
        "                    embedding_function=self.primary_embedding_func,\n",
        "                    metadata=metadata\n",
        "                )\n",
        "                logger.info(f\"새 컬렉션 '{collection_name}'을 생성했습니다.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"컬렉션 초기화 실패: {e}\")\n",
        "            raise\n",
        "\n",
        "    def should_process_file(self, file_path: str) -> bool:\n",
        "        \"\"\"파일 처리 필요 여부 확인\"\"\"\n",
        "        return self.hash_manager.should_process_file(file_path)\n",
        "\n",
        "    def upsert_chunks_incremental(self, file_name: str, chunks: List[Dict[str, Any]],\n",
        "                                 parent_chunk_map: Dict[str, str]):\n",
        "        \"\"\"증분 청크 업서트 - 중복 방지\"\"\"\n",
        "        if not chunks:\n",
        "            logger.warning(f\"{file_name}: 처리할 청크가 없습니다.\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"{file_name}: {len(chunks)}개 청크 증분 처리 시작\")\n",
        "\n",
        "        print(f\"    -> '{file_name}': 청크 {len(chunks)}개 DB 저장 시작...\")\n",
        "\n",
        "\n",
        "        # 기존 청크 확인 및 삭제\n",
        "        self._remove_existing_chunks(file_name)\n",
        "\n",
        "        # 새 청크 추가\n",
        "        successful_batches = 0\n",
        "        total_batches = (len(chunks) + self.config.BATCH_SIZE - 1) // self.config.BATCH_SIZE\n",
        "        chunk_ids = []\n",
        "\n",
        "        for batch_idx in range(0, len(chunks), self.config.BATCH_SIZE):\n",
        "            batch_chunks = chunks[batch_idx:batch_idx + self.config.BATCH_SIZE]\n",
        "\n",
        "            batch_num = (batch_idx // self.config.BATCH_SIZE) + 1\n",
        "            total_batches = (len(chunks) + self.config.BATCH_SIZE - 1) // self.config.BATCH_SIZE\n",
        "            print(f\"        -> 배치 {batch_num}/{total_batches} 저장 중...\")\n",
        "\n",
        "            try:\n",
        "                ids, documents, metadatas = self._prepare_enhanced_batch_data(\n",
        "                    file_name, batch_chunks, batch_idx\n",
        "                )\n",
        "\n",
        "                self.collection.upsert(ids=ids, documents=documents, metadatas=metadatas)\n",
        "                chunk_ids.extend(ids)\n",
        "                successful_batches += 1\n",
        "\n",
        "                if successful_batches % 5 == 0:\n",
        "                    logger.info(f\"  -> {successful_batches}/{total_batches} 배치 완료\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"  -> 배치 {batch_idx//self.config.BATCH_SIZE + 1} 저장 실패: {e}\")\n",
        "                continue\n",
        "\n",
        "        # 메타데이터 업데이트\n",
        "        file_path = os.path.join(self.config.DOCX_DIRECTORY, file_name)\n",
        "        self.hash_manager.update_file_metadata(file_path, len(chunks), 0)\n",
        "        self.hash_manager.register_chunks(file_path, chunk_ids, parent_chunk_map)\n",
        "\n",
        "        print(f\"  -> {file_name}: {successful_batches}/{total_batches} 배치 성공적으로 저장\")\n",
        "        logger.info(f\"  -> {file_name}: {successful_batches}/{total_batches} 배치 성공적으로 저장\")\n",
        "\n",
        "    def _remove_existing_chunks(self, file_name: str):\n",
        "        \"\"\"기존 파일의 청크 제거\"\"\"\n",
        "        try:\n",
        "            # 파일의 기존 청크 ID 조회\n",
        "            existing_results = self.collection.get(\n",
        "                where={\"source_file\": {\"$eq\": file_name}},\n",
        "                include=[\"metadatas\"]\n",
        "            )\n",
        "\n",
        "            if existing_results['ids']:\n",
        "                # 기존 청크 삭제\n",
        "                self.collection.delete(ids=existing_results['ids'])\n",
        "                logger.info(f\"{file_name}: {len(existing_results['ids'])}개 기존 청크 삭제\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"{file_name}: 기존 청크 삭제 중 오류 (무시됨): {e}\")\n",
        "\n",
        "    def _prepare_enhanced_batch_data(self, file_name: str, chunks: List[Dict[str, Any]],\n",
        "                                   batch_start_idx: int) -> Tuple[List[str], List[str], List[Dict[str, Any]]]:\n",
        "        \"\"\"향상된 배치 데이터 준비\"\"\"\n",
        "        ids, documents, metadatas = [], [], []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_index = batch_start_idx + i\n",
        "\n",
        "            # 개선된 ID 생성 - 청크 고유 ID 사용\n",
        "            unique_id = chunk.get('chunk_id', f\"{file_name}_{chunk_index:04d}_{int(time.time())}\")\n",
        "\n",
        "            ids.append(unique_id)\n",
        "            documents.append(chunk['content'])\n",
        "\n",
        "            # 향상된 메타데이터\n",
        "            structure_info = chunk.get('structure_info', {})\n",
        "\n",
        "            effective_timestamp = 0\n",
        "            publication_timestamp = 0\n",
        "            try:\n",
        "                if chunk.get('effective_date'):\n",
        "                    dt_obj = datetime.strptime(chunk['effective_date'].replace(' ', '').strip('.'), '%Y.%m.%d')\n",
        "                    effective_timestamp = int(dt_obj.timestamp())\n",
        "                if chunk.get('publication_date'):\n",
        "                    dt_obj = datetime.strptime(chunk['publication_date'].replace(' ', '').strip('.'), '%Y.%m.%d')\n",
        "                    publication_timestamp = int(dt_obj.timestamp())\n",
        "            except (ValueError, TypeError):\n",
        "                pass # 날짜 변환 실패 시 0으로 유지\n",
        "\n",
        "            metadata = {\n",
        "                \"source_file\": file_name,\n",
        "                \"file_hash\": chunk.get('file_hash', ''),\n",
        "                \"chunk_id\": unique_id,\n",
        "                \"article_title\": chunk.get('title', 'Unknown')[:200],\n",
        "                \"level\": max(0, min(10, structure_info.get('level', 4))),\n",
        "                \"char_count\": max(0, chunk.get('char_count', 0)),\n",
        "                \"word_count\": max(0, chunk.get('word_count', 0)),\n",
        "                \"chunk_index\": chunk_index,\n",
        "\n",
        "                # 법률 구조 정보\n",
        "                \"structure_type\": structure_info.get('type', 'unknown'),\n",
        "                \"structure_number\": structure_info.get('number', ''),\n",
        "                \"parent_ref\": structure_info.get('parent_ref', ''),\n",
        "                \"cross_refs\": json.dumps(structure_info.get('cross_refs', []), ensure_ascii=False),\n",
        "                \"hierarchy_path\": json.dumps(chunk.get('hierarchy_path', []), ensure_ascii=False),\n",
        "                \"original_chunk_id\": structure_info.get('original_chunk_id', ''),\n",
        "\n",
        "                # 하위 청크 정보\n",
        "                \"is_sub_chunk\": chunk.get('is_sub_chunk', False),\n",
        "                \"sub_index\": chunk.get('sub_index', 0),\n",
        "                \"parent_chunk_id\": chunk.get('parent_chunk_id', ''),\n",
        "\n",
        "                # 시간 메타데이터 추가\n",
        "                \"effective_date\": effective_timestamp,\n",
        "                \"publication_date\": publication_timestamp,\n",
        "                \"amendment_info\": chunk.get('amendment_info', ''),\n",
        "\n",
        "                # 타임스탬프\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "                \"last_updated\": datetime.now().isoformat()\n",
        "            }\n",
        "            metadatas.append(metadata)\n",
        "\n",
        "        return ids, documents, metadatas\n",
        "\n",
        "    def query_enhanced(self, query_text: str, n_results: int = 5,\n",
        "                      structure_filter: Optional[str] = None,\n",
        "                      level_filter: Optional[int] = None,\n",
        "                      date_filter: Optional[Dict[str, str]] = None,\n",
        "                      include_context: bool = True) -> Dict[str, Any]:\n",
        "        if not query_text or not query_text.strip():\n",
        "            logger.warning(\"빈 검색어입니다.\")\n",
        "            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            if count == 0:\n",
        "                logger.info(\"검색할 문서가 없습니다.\")\n",
        "                return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "            # 필터 조건 구성\n",
        "            where_clause = self._build_where_clause(structure_filter, level_filter, date_filter)\n",
        "\n",
        "            # 기본 검색\n",
        "            results = self.collection.query(\n",
        "                query_texts=[query_text.strip()],\n",
        "                n_results=min(max(1, n_results * 2), count),  # 더 많은 후보 확보\n",
        "                where=where_clause\n",
        "            )\n",
        "\n",
        "            if not results['ids'][0]:\n",
        "                return results\n",
        "\n",
        "            # 컨텍스트 강화\n",
        "            if include_context:\n",
        "                enhanced_results = self._enhance_results_with_context(results, n_results)\n",
        "                return enhanced_results\n",
        "            else:\n",
        "                # 결과 수 제한\n",
        "                for key in results:\n",
        "                    if isinstance(results[key], list) and results[key]:\n",
        "                        results[key] = [results[key][0][:n_results]]\n",
        "                return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"검색 중 오류 발생: {e}\")\n",
        "            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "    def _build_where_clause(self, structure_filter: Optional[str], level_filter: Optional[int], date_filter: Optional[Dict[str, str]]):\n",
        "        \"\"\"검색 필터 조건 구성\"\"\"\n",
        "        filters = []\n",
        "\n",
        "        if structure_filter:\n",
        "            filters.append({\"structure_type\": {\"$eq\": structure_filter}})\n",
        "\n",
        "        if level_filter and 1 <= level_filter <= 10:\n",
        "            filters.append({\"level\": {\"$eq\": level_filter}})\n",
        "\n",
        "        # 날짜 필터링 로직\n",
        "        if date_filter:\n",
        "            # ChromaDB는 문자열 비교를 지원하므로 YYYY-MM-DD 형식으로 날짜를 제공해야 함\n",
        "            for key, value_dict in date_filter.items():\n",
        "                if key in [\"effective_date\", \"publication_date\"]:\n",
        "                    # {\"$gte\": \"2025-01-01\"} 같은 딕셔너리의 값을 타임스탬프로 변환\n",
        "                    new_value_dict = {}\n",
        "                    for op, date_str in value_dict.items():\n",
        "                        try:\n",
        "                            # 쿼리는 'YYYY-MM-DD' 형식을 가정\n",
        "                            dt_object = datetime.strptime(date_str, '%Y-%m-%d')\n",
        "                            timestamp = int(dt_object.timestamp())\n",
        "                            new_value_dict[op] = timestamp\n",
        "                        except (ValueError, TypeError):\n",
        "                            continue # 잘못된 날짜 형식은 무시\n",
        "\n",
        "                    if new_value_dict:\n",
        "                        filters.append({key: new_value_dict})\n",
        "\n",
        "        if not filters:\n",
        "            return None\n",
        "        elif len(filters) == 1:\n",
        "            return filters[0]\n",
        "        else:\n",
        "            return {\"$and\": filters}\n",
        "\n",
        "    def _enhance_results_with_context(self, results: Dict[str, Any], n_results: int) -> Dict[str, Any]:\n",
        "        \"\"\"검색 결과에 컨텍스트 추가\"\"\"\n",
        "        enhanced_ids = []\n",
        "        enhanced_docs = []\n",
        "        enhanced_metas = []\n",
        "        enhanced_distances = []\n",
        "\n",
        "        added_chunks = set()\n",
        "\n",
        "        for i, (chunk_id, doc, meta, distance) in enumerate(zip(\n",
        "            results['ids'][0], results['documents'][0],\n",
        "            results['metadatas'][0], results['distances'][0]\n",
        "        )):\n",
        "            if len(enhanced_ids) >= n_results:\n",
        "                break\n",
        "\n",
        "            if chunk_id in added_chunks:\n",
        "                continue\n",
        "\n",
        "            # 메인 청크 추가\n",
        "            enhanced_ids.append(chunk_id)\n",
        "            enhanced_docs.append(doc)\n",
        "            enhanced_metas.append(meta)\n",
        "            enhanced_distances.append(distance)\n",
        "            added_chunks.add(chunk_id)\n",
        "\n",
        "            # 관련 청크 찾기 (부모/자식 관계)\n",
        "            if len(enhanced_ids) < n_results:\n",
        "                related_chunks = self._find_related_chunks(meta, added_chunks, n_results - len(enhanced_ids))\n",
        "\n",
        "                for rel_id, rel_doc, rel_meta, rel_dist in related_chunks:\n",
        "                    if len(enhanced_ids) >= n_results:\n",
        "                        break\n",
        "                    enhanced_ids.append(rel_id)\n",
        "                    enhanced_docs.append(rel_doc)\n",
        "                    enhanced_metas.append(rel_meta)\n",
        "                    enhanced_distances.append(rel_dist + 0.1)  # 약간 낮은 우선순위\n",
        "                    added_chunks.add(rel_id)\n",
        "\n",
        "        return {\n",
        "            'ids': [enhanced_ids],\n",
        "            'documents': [enhanced_docs],\n",
        "            'metadatas': [enhanced_metas],\n",
        "            'distances': [enhanced_distances]\n",
        "        }\n",
        "\n",
        "    def _find_related_chunks(self, main_meta: Dict[str, Any], added_chunks: Set[str], max_related: int) -> List[Tuple]:\n",
        "        \"\"\"관련 청크 찾기\"\"\"\n",
        "        related = []\n",
        "\n",
        "        try:\n",
        "            # 부모 청크 찾기 (하위 청크인 경우)\n",
        "            if main_meta.get('is_sub_chunk') and main_meta.get('parent_chunk_id'):\n",
        "                parent_id = main_meta['parent_chunk_id']\n",
        "                if parent_id not in added_chunks:\n",
        "                    parent_result = self.collection.get(\n",
        "                        ids=[parent_id],\n",
        "                        include=['documents', 'metadatas']\n",
        "                    )\n",
        "                    if parent_result['ids']:\n",
        "                        related.append((\n",
        "                            parent_result['ids'][0],\n",
        "                            parent_result['documents'][0],\n",
        "                            parent_result['metadatas'][0],\n",
        "                            0.05  # 부모는 높은 관련성\n",
        "                        ))\n",
        "\n",
        "            # 자식 청크들 찾기 (부모 청크인 경우)\n",
        "            if not main_meta.get('is_sub_chunk'):\n",
        "                chunk_id = main_meta.get('chunk_id')\n",
        "                if chunk_id:\n",
        "                    children_result = self.collection.get(\n",
        "                        where={\"parent_chunk_id\": {\"$eq\": chunk_id}},\n",
        "                        include=['documents', 'metadatas'],\n",
        "                        limit=max_related\n",
        "                    )\n",
        "                    for child_id, child_doc, child_meta in zip(\n",
        "                        children_result['ids'], children_result['documents'], children_result['metadatas']\n",
        "                    ):\n",
        "                        if child_id not in added_chunks and len(related) < max_related:\n",
        "                            related.append((child_id, child_doc, child_meta, 0.08))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"관련 청크 검색 중 오류: {e}\")\n",
        "\n",
        "        return related[:max_related]\n",
        "\n",
        "    def get_comprehensive_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"포괄적인 통계 정보\"\"\"\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            if count == 0:\n",
        "                return {\"total_chunks\": 0, \"message\": \"데이터가 없습니다.\"}\n",
        "\n",
        "            # 전체 메타데이터 샘플링\n",
        "            sample_size = min(2000, count)\n",
        "            sample = self.collection.get(\n",
        "                limit=sample_size,\n",
        "                include=['metadatas']\n",
        "            )\n",
        "\n",
        "            if not sample['metadatas']:\n",
        "                return {\"total_chunks\": count, \"message\": \"메타데이터가 없습니다.\"}\n",
        "\n",
        "            # 통계 계산\n",
        "            stats = self._calculate_detailed_stats(sample['metadatas'], count)\n",
        "\n",
        "            # 파일별 통계 추가\n",
        "            file_stats = self._get_file_statistics()\n",
        "            stats.update(file_stats)\n",
        "\n",
        "            return stats\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"통계 생성 중 오류: {e}\")\n",
        "            return {\"total_chunks\": 0, \"error\": str(e)}\n",
        "\n",
        "    def _calculate_detailed_stats(self, metadatas: List[Dict], total_count: int) -> Dict[str, Any]:\n",
        "        \"\"\"상세 통계 계산\"\"\"\n",
        "        char_counts = []\n",
        "        structure_counts = defaultdict(int)\n",
        "        level_counts = defaultdict(int)\n",
        "        files = set()\n",
        "        sub_chunk_count = 0\n",
        "        cross_ref_count = 0\n",
        "        temporal_data_count = defaultdict(int)\n",
        "\n",
        "        for meta in metadatas:\n",
        "            if meta.get(\"source_file\"):\n",
        "                files.add(meta[\"source_file\"])\n",
        "\n",
        "            char_count = meta.get(\"char_count\", 0)\n",
        "            if isinstance(char_count, (int, float)):\n",
        "                char_counts.append(char_count)\n",
        "\n",
        "            structure_type = meta.get(\"structure_type\", \"unknown\")\n",
        "            structure_counts[structure_type] += 1\n",
        "\n",
        "            level = meta.get(\"level\")\n",
        "            if isinstance(level, (int, float)):\n",
        "                level_counts[int(level)] += 1\n",
        "\n",
        "            if meta.get(\"is_sub_chunk\", False):\n",
        "                sub_chunk_count += 1\n",
        "\n",
        "            cross_refs = meta.get(\"cross_refs\", \"[]\")\n",
        "            try:\n",
        "                refs = json.loads(cross_refs) if isinstance(cross_refs, str) else cross_refs\n",
        "                if refs:\n",
        "                    cross_ref_count += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # 시간 메타데이터 통계\n",
        "            if meta.get(\"effective_date\"):\n",
        "                temporal_data_count[\"effective_date\"] += 1\n",
        "            if meta.get(\"publication_date\"):\n",
        "                temporal_data_count[\"publication_date\"] += 1\n",
        "            if meta.get(\"amendment_info\"):\n",
        "                temporal_data_count[\"amendment_info\"] += 1\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"total_chunks\": total_count,\n",
        "            \"sampled_chunks\": len(metadatas),\n",
        "            \"files\": len(files),\n",
        "            \"avg_char_count\": sum(char_counts) / max(1, len(char_counts)) if char_counts else 0,\n",
        "            \"min_char_count\": min(char_counts) if char_counts else 0,\n",
        "            \"max_char_count\": max(char_counts) if char_counts else 0,\n",
        "            \"structure_distribution\": dict(structure_counts),\n",
        "            \"level_distribution\": dict(level_counts),\n",
        "            \"sub_chunks\": sub_chunk_count,\n",
        "            \"complete_structures\": len(metadatas) - sub_chunk_count,\n",
        "            \"chunks_with_cross_refs\": cross_ref_count,\n",
        "            \"cross_ref_percentage\": (cross_ref_count / max(1, len(metadatas))) * 100,\n",
        "            \"temporal_data_counts\": dict(temporal_data_count),\n",
        "        }\n",
        "\n",
        "    def _get_file_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"파일별 통계\"\"\"\n",
        "        try:\n",
        "            with sqlite3.connect(self.hash_manager.db_path) as conn:\n",
        "                cursor = conn.execute('''\n",
        "                    SELECT COUNT(*) as file_count,\n",
        "                           SUM(chunk_count) as total_chunks,\n",
        "                           AVG(chunk_count) as avg_chunks_per_file,\n",
        "                           AVG(processing_time) as avg_processing_time\n",
        "                    FROM file_metadata\n",
        "                ''')\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if result:\n",
        "                    return {\n",
        "                        \"processed_files\": result[0] or 0,\n",
        "                        \"total_chunks_from_files\": result[1] or 0,\n",
        "                        \"avg_chunks_per_file\": result[2] or 0,\n",
        "                        \"avg_processing_time\": result[3] or 0\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"파일 통계 조회 실패: {e}\")\n",
        "\n",
        "        return {\"processed_files\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYMJRMv-zo2n"
      },
      "outputs": [],
      "source": [
        "def process_file_parallel(args: Tuple[str, 'Config']) -> Tuple[str, bool, int, float, Optional[str]]:\n",
        "    file_path, config = args\n",
        "    file_name = os.path.basename(file_path)\n",
        "    start_time = time.time()\n",
        "    logger = logging.getLogger(__name__) # Get logger for the process\n",
        "\n",
        "    try:\n",
        "        hash_manager = FileHashManager(config.METADATA_DB_PATH)\n",
        "\n",
        "        if not hash_manager.should_process_file(file_path):\n",
        "            logger.info(f\"{file_name}: No changes detected, skipping.\")\n",
        "            return file_name, True, 0, 0, None\n",
        "\n",
        "        processor = EnhancedDocumentProcessor(file_path, hash_manager)\n",
        "        chunks, parent_chunk_map = processor.extract_structured_chunks()\n",
        "\n",
        "        if not chunks:\n",
        "            logger.warning(f\"{file_name}: No chunks were extracted.\")\n",
        "            return file_name, False, 0, time.time() - start_time, None\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Save results to a temporary file for the main process\n",
        "        temp_dir = os.path.join(os.getcwd(), \"temp_chunks\")\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        temp_file_path = os.path.join(temp_dir, f\"chunks_{uuid.uuid4()}.pkl\")\n",
        "\n",
        "        with open(temp_file_path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'chunks': chunks,\n",
        "                'parent_chunk_map': parent_chunk_map,\n",
        "                'file_name': file_name,\n",
        "                'file_path': file_path,\n",
        "                'processing_time': processing_time\n",
        "            }, f)\n",
        "\n",
        "        return file_name, True, len(chunks), processing_time, temp_file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {file_name} in parallel: {e}\", exc_info=True)\n",
        "        return file_name, False, 0, time.time() - start_time, None\n",
        "\n",
        "def main():\n",
        "\n",
        "    try:\n",
        "        config = Config()\n",
        "        logger.info(f\"Data directory: {config.DOCX_DIRECTORY}\")\n",
        "        logger.info(f\"Max workers: {config.MAX_WORKERS}\")\n",
        "        logger.info(f\"Parallel processing enabled: {config.ENABLE_PARALLEL}\")\n",
        "\n",
        "        # config.COLLECTION_NAME = \"legal_docs\"\n",
        "\n",
        "        print(config.COLLECTION_NAME)\n",
        "\n",
        "        data_path = Path(config.DOCX_DIRECTORY)\n",
        "        if not data_path.exists():\n",
        "            data_path.mkdir(parents=True, exist_ok=True)\n",
        "            logger.info(f\"Created directory '{config.DOCX_DIRECTORY}'. Please add document files and run again.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # 지원하는 모든 파일 확장자 포함\n",
        "            supported_extensions = (\".docx\", \".pdf\", \".pptx\")\n",
        "            files_to_process = [\n",
        "                os.path.join(config.DOCX_DIRECTORY, f)\n",
        "                for f in os.listdir(config.DOCX_DIRECTORY)\n",
        "                if f.lower().endswith(supported_extensions) and not f.startswith(\"~\")\n",
        "            ]\n",
        "        except PermissionError:\n",
        "            logger.error(f\"Permission denied for directory '{config.DOCX_DIRECTORY}'.\")\n",
        "            return\n",
        "\n",
        "        if not files_to_process:\n",
        "            logger.info(f\"No '.docx', '.pdf', or '.pptx' files found in '{config.DOCX_DIRECTORY}'.\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Found {len(files_to_process)} files to process.\")\n",
        "\n",
        "        db_manager = EnhancedVectorDBManager(config)\n",
        "        total_chunks_processed = 0\n",
        "        successful_files = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        if config.ENABLE_PARALLEL and len(files_to_process) > 1:\n",
        "            logger.info(\"Running in parallel processing mode.\")\n",
        "            temp_result_files = []\n",
        "\n",
        "            with ProcessPoolExecutor(max_workers=config.MAX_WORKERS) as executor:\n",
        "                file_args = [(file_path, config) for file_path in files_to_process]\n",
        "                futures = {executor.submit(process_file_parallel, args): args[0] for args in file_args}\n",
        "\n",
        "                for i, future in enumerate(as_completed(futures)):\n",
        "                    file_name, success, num_chunks, p_time, temp_file = future.result()\n",
        "                    if success:\n",
        "                        print(f\"--- [{i+1}/{len(files_to_process)}] 파일 처리 완료: {file_name} ({p_time:.2f}s, {num_chunks} chunks) ---\")\n",
        "                        if temp_file:\n",
        "                            temp_result_files.append(temp_file)\n",
        "                        logger.info(f\"[{i+1}/{len(files_to_process)}] File processed: {file_name} ({p_time:.2f}s, {num_chunks} chunks)\")\n",
        "                    else:\n",
        "                        print(f\"--- [{i+1}/{len(files_to_process)}] 파일 처리 실패: {file_name} ---\")\n",
        "                        logger.error(f\"[{i+1}/{len(files_to_process)}] Failed to process file: {file_name}\")\n",
        "\n",
        "            logger.info(\"Chunk extraction complete. Starting database ingestion...\")\n",
        "            for temp_file in temp_result_files:\n",
        "                try:\n",
        "                    with open(temp_file, 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "\n",
        "                    db_manager.upsert_chunks_incremental(data['file_name'], data['chunks'], data['parent_chunk_map'])\n",
        "                    total_chunks_processed += len(data['chunks'])\n",
        "                    successful_files += 1\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to ingest data from {temp_file}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(temp_file)\n",
        "\n",
        "            temp_dir = os.path.join(os.getcwd(), \"temp_chunks\")\n",
        "            if os.path.exists(temp_dir):\n",
        "                try:\n",
        "                    os.rmdir(temp_dir)\n",
        "                except OSError:\n",
        "                    pass # Directory might not be empty if an error occurred\n",
        "\n",
        "        else: # Sequential processing\n",
        "            logger.info(\"Running in sequential processing mode.\")\n",
        "            for i, file_path in enumerate(files_to_process):\n",
        "                file_name = os.path.basename(file_path)\n",
        "                print(f\"\\n--- [{i+1}/{len(files_to_process)}] 파일 처리 시작: {file_name} ---\")\n",
        "                logger.info(f\"[{i+1}/{len(files_to_process)}] Processing: {file_name}\")\n",
        "                p_start_time = time.time()\n",
        "\n",
        "                if not db_manager.should_process_file(file_path):\n",
        "                    logger.info(f\"  -> No changes detected, skipping.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    processor = EnhancedDocumentProcessor(file_path, db_manager.hash_manager)\n",
        "                    chunks, parent_chunk_map = processor.extract_structured_chunks()\n",
        "\n",
        "                    if chunks:\n",
        "                        db_manager.upsert_chunks_incremental(file_name, chunks, parent_chunk_map)\n",
        "                        p_time = time.time() - p_start_time\n",
        "                        total_chunks_processed += len(chunks)\n",
        "                        successful_files += 1\n",
        "                        logger.info(f\"  -> Completed in {p_time:.2f}s, found {len(chunks)} chunks.\")\n",
        "\n",
        "                    else:\n",
        "                        logger.warning(f\"  -> No chunks were extracted from {file_name}.\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"  -> An error occurred while processing {file_name}: {e}\", exc_info=True)\n",
        "\n",
        "        # --- Final Report ---\n",
        "        total_time = time.time() - start_time\n",
        "        logger.info(\"\\n\" + \"=\"*20 + \" Processing Complete \" + \"=\"*20)\n",
        "        logger.info(f\"Total execution time: {total_time:.2f} seconds\")\n",
        "        logger.info(f\"Successfully processed files: {successful_files}/{len(files_to_process)}\")\n",
        "        logger.info(f\"Total chunks ingested/updated: {total_chunks_processed}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*20 + \" Processing Complete \" + \"=\"*20)\n",
        "        print(f\"Total execution time: {total_time:.2f} seconds\")\n",
        "        print(f\"Successfully processed files: {successful_files}/{len(files_to_process)}\")\n",
        "        print(f\"Total chunks ingested/updated: {total_chunks_processed}\")\n",
        "\n",
        "        stats = db_manager.get_comprehensive_statistics()\n",
        "        logger.info(f\"Database Statistics: {json.dumps(stats, ensure_ascii=False, indent=2)}\")\n",
        "\n",
        "        # --- Enhanced Search Test ---\n",
        "        if stats.get(\"total_chunks\", 0) > 0:\n",
        "            logger.info(\"\\n\" + \"=\"*20 + \" Enhanced Search Test \" + \"=\"*20)\n",
        "            test_queries = [\n",
        "                {\"query\": \"사장 임명\", \"structure_filter\": \"article\"},\n",
        "                {\"query\": \"항만시설 사용을 위한 허가\"},\n",
        "                {\"query\": \"준공확인\", \"level_filter\": 3},\n",
        "                {\"query\": \"사용료\", \"include_context\": False},\n",
        "                {\"query\": \"사용료\", \"date_filter\": {\"effective_date\": {\"$gte\": \"2025-01-01\"}}}\n",
        "            ]\n",
        "\n",
        "            for test_case in test_queries:\n",
        "                query = test_case[\"query\"]\n",
        "                options = {k:v for k,v in test_case.items() if k != 'query'}\n",
        "                logger.info(f\"\\n[Query]: '{query}' (Options: {options})\")\n",
        "                print(f\"\\n[Query]: '{query}' (Options: {options})\")\n",
        "\n",
        "                try:\n",
        "                    results = db_manager.query_enhanced(\n",
        "                        query_text=query,\n",
        "                        n_results=3,\n",
        "                        structure_filter=test_case.get(\"structure_filter\"),\n",
        "                        level_filter=test_case.get(\"level_filter\"),\n",
        "                        date_filter=test_case.get(\"date_filter\"),\n",
        "                        include_context=test_case.get(\"include_context\", True)\n",
        "                    )\n",
        "\n",
        "                    if results and results['ids'] and results['ids'][0]:\n",
        "                        for i, (meta, doc, dist) in enumerate(zip(results['metadatas'][0], results['documents'][0], results['distances'][0])):\n",
        "                            context_info = \"\"\n",
        "                            if meta.get('is_sub_chunk'):\n",
        "                                context_info = f\"(Sub-chunk of: ...{meta.get('parent_chunk_id', 'N/A')[-15:]})\"\n",
        "\n",
        "                            temporal_info = f\"시행일: {meta.get('effective_date', 'N/A')}\"\n",
        "\n",
        "                            logger.info(f\"  [{i+1}] Title: {meta.get('article_title', 'N/A')} {context_info}\")\n",
        "                            print(f\"  [{i+1}] Title: {meta.get('article_title', 'N/A')} {context_info}\")\n",
        "                            logger.info(f\"      - Similarity: {1 - dist:.4f} | Structure: {meta.get('structure_type')}:{meta.get('structure_number')} | {temporal_info}\")\n",
        "                            logger.info(f\"      - Source: {meta.get('source_file')}\")\n",
        "                            logger.info(f\"      - Content: {doc[:120].replace(chr(10), ' ')}...\")\n",
        "                            print(f\"      - Similarity: {1 - dist:.4f} | Structure: {meta.get('structure_type')}:{meta.get('structure_number')} | {temporal_info}\")\n",
        "                            print(f\"      - Source: {meta.get('source_file')}\")\n",
        "                            print(f\"      - Content: {doc[:120].replace(chr(10), ' ')}...\")\n",
        "\n",
        "                    else:\n",
        "                        logger.info(\"  -> No results found.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"  -> An error occurred during search test: {e}\", exc_info=True)\n",
        "        else:\n",
        "            logger.warning(\"No chunks in DB, skipping search test.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"\\nProcess interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred in the main process: {e}\", exc_info=True)\n",
        "        sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_CAla-EuA5D",
        "outputId": "069ed1c3-21b4-407c-e77f-64673352c3b7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "legal_manuals\n",
            "    -> '항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf': 청크 추출 및 분석 시작...\n",
            "    -> '항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf': 청크 96개 추출 완료.\n",
            "--- [1/2] 파일 처리 완료: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf (10.07s, 96 chunks) ---\n",
            "    -> '해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx': 청크 추출 및 분석 시작...\n",
            "    -> '해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx': 청크 426개 추출 완료.\n",
            "--- [2/2] 파일 처리 완료: 해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx (16.71s, 426 chunks) ---\n",
            "    -> '항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf': 청크 96개 DB 저장 시작...\n",
            "        -> 배치 1/1 저장 중...\n",
            "  -> 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf: 1/1 배치 성공적으로 저장\n",
            "    -> '해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx': 청크 426개 DB 저장 시작...\n",
            "        -> 배치 1/5 저장 중...\n",
            "        -> 배치 2/5 저장 중...\n",
            "        -> 배치 3/5 저장 중...\n",
            "        -> 배치 4/5 저장 중...\n",
            "        -> 배치 5/5 저장 중...\n",
            "  -> 해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx: 5/5 배치 성공적으로 저장\n",
            "\n",
            "==================== Processing Complete ====================\n",
            "Total execution time: 234.75 seconds\n",
            "Successfully processed files: 2/2\n",
            "Total chunks ingested/updated: 522\n",
            "\n",
            "[Query]: '사장 임명' (Options: {'structure_filter': 'article'})\n",
            "  [1] Title: article:93 권한의 위임 ①해양수산부장관은 법 제104조제1항에 따라 법 제3조제2항제1호 \n",
            "      - Similarity: 0.2269 | Structure: article:93 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [article:93 권한의 위임 ①해양수산부장관은 법 제104조제1항에 따라 법 제3조제2항제1호]\\n제93조(권한의 위임) ①해양수산부장관은 법 제104조제1항에 따라 법 제3조제2항제1호 및 같은 조 제3항제1...\n",
            "  [2] Title: article:23 출입통제구역의 지정 지방해양수산청장 또는 시・도지사는 법 제28조제2항 \n",
            "      - Similarity: 0.1966 | Structure: article:23 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [article:23 출입통제구역의 지정 지방해양수산청장 또는 시・도지사는 법 제28조제2항]\\n제23조(출입통제구역의 지정) 지방해양수산청장 또는 시・도지사는 법 제28조제2항 각 호의 장소를 출입통제구역으로 지정...\n",
            "  [3] Title: article:104 권한 등의 위임·위탁 ① 이 법에 따른 해양수산부장관의 권한은 대통령령으로 \n",
            "      - Similarity: 0.1925 | Structure: article:104 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [article:104 권한 등의 위임·위탁 ① 이 법에 따른 해양수산부장관의 권한은 대통령령으로]\\n제104조(권한 등의 위임·위탁) ① 이 법에 따른 해양수산부장관의 권한은 대통령령으로 정하는 바에 따라 그 일부...\n",
            "\n",
            "[Query]: '항만시설 사용을 위한 허가' (Options: {})\n",
            "  [1] Title: chapter:5 참고자료 (부분 1) (Sub-chunk of: ...0150_1754381639)\n",
            "      - Similarity: 0.6272 | Structure: chapter:5 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [chapter:5 참고자료 (부분)]\\n[chapter:5 참고자료]\\n제5장 참고자료 Ž 항만구역 내 출입통제구역 지정 현황 £ 출입통제구역 지정 현황(2023년 12월 기준) - (지방청) 출입통제구역 지정 현...\n",
            "  [2] Title: chapter:3 항만건설공사 주요 공종의 안전관리업무 (부분 2) (Sub-chunk of: ...0193_1754381651)\n",
            "      - Similarity: 0.6151 | Structure: chapter:3 | 시행일: 0\n",
            "      - Source: 해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx\n",
            "      - Content: [chapter:3 항만건설공사 주요 공종의 안전관리업무 (부분)]\\n매립지  매립지의 울타리 구조물은 견고한 구조로 빈틈없이, 또한 적정한 높이로 축조한다. 여수토출구 또는 토운선 등 선박 출입구 부근에는 필요에 ...\n",
            "  [3] Title: paragraph:5 ⑦ 번 호 번 호 년월일        납 액 ⑨    ⑩     사 항      (인) \n",
            "      - Similarity: 0.5994 | Structure: paragraph:5 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [paragraph:5 ⑦ 번 호 번 호 년월일        납 액 ⑨    ⑩     사 항      (인)]\\n⑤  ⑦ 번 호 번 호 년월일        납 액 ⑨    ⑩     사 항      (인) 납 부 ...\n",
            "\n",
            "[Query]: '준공확인' (Options: {'level_filter': 3})\n",
            "  [1] Title: article:23 출입통제구역의 지정 지방해양수산청장 또는 시・도지사는 법 제28조제2항 \n",
            "      - Similarity: 0.3769 | Structure: article:23 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [article:23 출입통제구역의 지정 지방해양수산청장 또는 시・도지사는 법 제28조제2항]\\n제23조(출입통제구역의 지정) 지방해양수산청장 또는 시・도지사는 법 제28조제2항 각 호의 장소를 출입통제구역으로 지정...\n",
            "  [2] Title: article:16 사전통지 및 의견 제출 등 ① 행정청이 질서위반행위에 대하여 과태료를 \n",
            "      - Similarity: 0.3133 | Structure: article:16 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [article:16 사전통지 및 의견 제출 등 ① 행정청이 질서위반행위에 대하여 과태료를]\\n제16조(사전통지 및 의견 제출 등) ① 행정청이 질서위반행위에 대하여 과태료를 부과하고자 하는 때에는 미리 당사자(제1...\n",
            "  [3] Title: article:3 사전통지 및 의견제출 등 ① 법 제16조 제1항에 따라 행정청이 과태료 부과에 \n",
            "      - Similarity: 0.2862 | Structure: article:3 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [article:3 사전통지 및 의견제출 등 ① 법 제16조 제1항에 따라 행정청이 과태료 부과에]\\n제3조(사전통지 및 의견제출 등) ① 법 제16조 제1항에 따라 행정청이 과태료 부과에 관하여 미리 통지하는 경우...\n",
            "\n",
            "[Query]: '사용료' (Options: {'include_context': False})\n",
            "  [1] Title: subitem:라 스마트 안전장비 대가 산정 (부분 1) (Sub-chunk of: ...0058_1754381651)\n",
            "      - Similarity: 0.4868 | Structure: subitem:라 | 시행일: 0\n",
            "      - Source: 해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx\n",
            "      - Content: [subitem:라 스마트 안전장비 대가 산정 (부분)]\\n[subitem:라 스마트 안전장비 대가 산정]\\n라. 스마트 안전장비 대가 산정  < 스마트 안전장비 대가 산정 > | 구 분 |  | 근\t거 | | --...\n",
            "  [2] Title: subitem:라 스마트 안전장비 대가 산정 (부분 2) (Sub-chunk of: ...0058_1754381651)\n",
            "      - Similarity: 0.4767 | Structure: subitem:라 | 시행일: 0\n",
            "      - Source: 해양수산부_항만건설 안전사고 예방 매뉴얼_20231201.pptx\n",
            "      - Content: [subitem:라 스마트 안전장비 대가 산정 (부분)]\\n< 스마트 안전장비 운영비 대가 산정표 > | 구 분 |  | 대\t가 | 비\t고 | | --- | --- | --- | --- | | 운영비 | 관제 | 기...\n",
            "  [3] Title: paragraph:5 ⑦ 번 호 번 호 년월일        납 액 ⑨    ⑩     사 항      (인) \n",
            "      - Similarity: 0.4484 | Structure: paragraph:5 | 시행일: 0\n",
            "      - Source: 항만구역 내 출입통제구역 지정·운영 매뉴얼(해양수산부, 2024.1).pdf\n",
            "      - Content: [paragraph:5 ⑦ 번 호 번 호 년월일        납 액 ⑨    ⑩     사 항      (인)]\\n⑤  ⑦ 번 호 번 호 년월일        납 액 ⑨    ⑩     사 항      (인) 납 부 ...\n",
            "\n",
            "[Query]: '사용료' (Options: {'date_filter': {'effective_date': {'$gte': '2025-01-01'}}})\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    mp.freeze_support()\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import chromadb\n",
        "import logging\n",
        "\n",
        "# 이 셀을 실행하기 전에 노트북의 다른 셀에서\n",
        "# Config 클래스가 모두 정의되어 있어야 합니다.\n",
        "# 로거가 정의되지 않은 경우를 대비한 기본 설정\n",
        "if 'logger' not in globals():\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    logger = logging.getLogger(\"VerificationTest\")\n",
        "\n",
        "\n",
        "def verify_main_pipeline_results():\n",
        "    \"\"\"\n",
        "    이전 셀들에서 실행된 메인 파이프라인의 결과로 생성된\n",
        "    ChromaDB 컬렉션들을 검증합니다.\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"  메인 파이프라인 실행 결과(DB 컬렉션) 검증을 시작합니다.\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. 원본 Config 클래스에서 DB 경로를 가져옵니다.\n",
        "    #    이렇게 하면 원본 코드의 설정이 변경되어도 테스트 코드를 수정할 필요가 없습니다.\n",
        "    try:\n",
        "        original_db_path = Config.DB_PATH\n",
        "        # 사용자가 순차적으로 실행했다고 가정한 검증 대상 컬렉션 목록\n",
        "        expected_collections = [\"legal_manuals\", \"legal_docs\"]\n",
        "    except NameError:\n",
        "        print(\"❌ 검증 실패: 'Config' 클래스가 정의되지 않았습니다.\")\n",
        "        print(\"이전 셀들을 먼저 실행하여 Config 클래스를 메모리에 로드해주세요.\")\n",
        "        return\n",
        "\n",
        "    all_tests_passed = True\n",
        "    print(f\"검증 대상 DB 경로: {original_db_path}\")\n",
        "\n",
        "    # 2. DB 경로 존재 여부 확인\n",
        "    if not os.path.exists(original_db_path):\n",
        "        print(f\"❌ 검증 실패: DB 경로 '{original_db_path}'를 찾을 수 없습니다.\")\n",
        "        print(\"메인 파이프라인을 먼저 실행하여 DB를 생성해주세요.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 3. DB에 연결하고 모든 컬렉션 목록 가져오기\n",
        "        client = chromadb.PersistentClient(path=original_db_path)\n",
        "        existing_collections = [col.name for col in client.list_collections()]\n",
        "        print(f\"현재 DB에 존재하는 컬렉션: {existing_collections}\")\n",
        "\n",
        "        # 4. 기대하는 각 컬렉션이 존재하는지 확인\n",
        "        for collection_name in expected_collections:\n",
        "            print(f\"\\n--- '{collection_name}' 컬렉션 존재 여부 확인 ---\")\n",
        "            if collection_name in existing_collections:\n",
        "                print(f\"✅ 성공: '{collection_name}' 컬렉션이 DB에 존재합니다.\")\n",
        "                # 추가적으로 컬렉션의 데이터 수 확인\n",
        "                collection = client.get_collection(name=collection_name)\n",
        "                count = collection.count()\n",
        "                print(f\"  -> '{collection_name}' 컬렉션의 데이터(청크) 수: {count}\")\n",
        "                if count == 0:\n",
        "                     print(f\"  ⚠️ 경고: 컬렉션은 존재하지만 데이터가 없습니다.\")\n",
        "            else:\n",
        "                print(f\"❌ 실패: '{collection_name}' 컬렉션을 찾을 수 없습니다.\")\n",
        "                all_tests_passed = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류: DB 검증 중 예외가 발생했습니다: {e}\")\n",
        "        logger.error(\"DB 검증 실패\", exc_info=True)\n",
        "        all_tests_passed = False\n",
        "\n",
        "    # 5. 최종 결과 요약\n",
        "    print(\"\\n\" + \"=\"*25 + \" 검증 결과 요약 \" + \"=\"*25)\n",
        "    if all_tests_passed:\n",
        "        print(\"🎉 모든 필수 컬렉션이 성공적으로 검증되었습니다! 🎉\")\n",
        "    else:\n",
        "        print(\"🔥 일부 컬렉션을 찾지 못했습니다. 위 로그를 확인해주세요. 🔥\")\n",
        "    print(\"=\"*61)\n",
        "\n",
        "\n",
        "# --- 검증 실행 ---\n",
        "verify_main_pipeline_results()\n"
      ],
      "metadata": {
        "id": "RmwyLN6OkL9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b89cae-c6b1-42fc-960a-a33fc5d0eb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  메인 파이프라인 실행 결과(DB 컬렉션) 검증을 시작합니다.\n",
            "============================================================\n",
            "검증 대상 DB 경로: /content/chroma_db\n",
            "현재 DB에 존재하는 컬렉션: ['legal_manuals', 'legal_docs']\n",
            "\n",
            "--- 'legal_manuals' 컬렉션 존재 여부 확인 ---\n",
            "✅ 성공: 'legal_manuals' 컬렉션이 DB에 존재합니다.\n",
            "  -> 'legal_manuals' 컬렉션의 데이터(청크) 수: 522\n",
            "\n",
            "--- 'legal_docs' 컬렉션 존재 여부 확인 ---\n",
            "✅ 성공: 'legal_docs' 컬렉션이 DB에 존재합니다.\n",
            "  -> 'legal_docs' 컬렉션의 데이터(청크) 수: 32193\n",
            "\n",
            "========================= 검증 결과 요약 =========================\n",
            "🎉 모든 필수 컬렉션이 성공적으로 검증되었습니다! 🎉\n",
            "=============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}